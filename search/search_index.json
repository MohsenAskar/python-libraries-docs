{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Python Libraries by Mohsen Askar","text":"<p>Welcome to the documentation hub for my Python libraries. Here you'll find comprehensive documentation for:</p>"},{"location":"#modularity-encoding","title":"Modularity Encoding","text":"<p><code>modularity_encoding</code> package provides functionality for grouping high dimensional Health Coding Systems (HCSs),such as ICD and ATC codes, in Machine Learning Models and similar applications.</p> <p>PyPi address: https://pypi.org/project/modularity-encoding/</p>"},{"location":"#stata-codebook","title":"Stata Codebook","text":"<p><code>stata_codebook</code> provides tools for generating detailed descriptive statistics and summaries of data frames, similar to Stata's <code>codebook</code> command.</p> <p>PyPi address: https://pypi.org/project/stata-codebook/</p>"},{"location":"#my-dataviewer-gui","title":"My DataViewer GUI","text":"<p><code>mydataViewer_GUI</code> is a Lightweight Real-Time Interactive Data Viewer IDE for Python inspired by the Stata <code>browser</code> command.</p> <p>PyPi address: https://pypi.org/project/mydataviewer-GUI/</p>"},{"location":"#assumption-sheriff","title":"Assumption Sheriff","text":"<p><code>AssumptionSheriff</code> is a comprehensive Python package designed to automatically validate statistical tests' assumptions.</p> <p>PyPi address: https://pypi.org/project/assumption-sheriff/</p>"},{"location":"#data-insightron","title":"Data Insightron","text":"<p><code>DataInsightron</code> is a Python package designed to provide a comprehensive overview of a dataset characteristics.</p> <p>PyPi address: https://pypi.org/project/data_insightron/</p>"},{"location":"#audio2topics","title":"Audio2Topics","text":"<p><code>Audio2Topics</code> is a Python package to automatically extract topics' suggstions from voice/text files.</p> <p>PyPi address: https://pypi.org/project/audio2topics/</p> <p>Each library's documentation includes:</p> <ul> <li>Getting started guide</li> <li>Usage examples</li> <li>Related information</li> </ul> <p>Stay tuned for more packages! \ud83d\ude42</p>"},{"location":"assumption_sheriff/","title":"Overview","text":""},{"location":"assumption_sheriff/#assumption-sheriff-package","title":"<code>Assumption Sheriff</code> Package \ud83d\udc6e\u200d\u2642\ufe0f\u2b50","text":"<p><code>AssumptionSheriff</code> is a comprehensive Python package designed to validate statistical test assumptions. It provides automated checking of common statistical assumptions and offers recommendations when assumptions are violated. The packges supports 16 commonly used statistical tests, more tests will be added in the coming package updates.</p> <p>Unfortunately, it is quite common that published health research do not fully report or validate the underlying assumptions of statistical tests utilized as reported by many articles, see: </p> <ul> <li> <p>Hoekstra et al. (2012) \"Are Assumptions of Well-Known Statistical Techniques Checked, and Why (Not)?\" https://doi.org/10.3389/fpsyg.2012.00137</p> </li> <li> <p>Patino et. al (2018) \"Meeting the assumptions of statistical tests: an important and often forgotten step to reporting valid results\" https://doi.org/10.1590/S1806-37562018000000303</p> </li> <li> <p>Nielsen et. al (2019) \"Assessing assumptions for statistical analyses in randomised clinical trials\" https://doi.org/10.1136/bmjebm-2019-111174</p> </li> </ul> <p>To bridge this gap, <code>Assumption Sheriff</code>  automates the process of checking statistical test assumptions, providing clear feedback and suggestions for alternative approaches when violations occur.</p>"},{"location":"assumption_sheriff/#supported-statistical-tests","title":"Supported Statistical Tests","text":"<p><code>AssumptionSheriff</code> supports assumption checking for the following statistical tests:</p> <ol> <li>Independent Samples t-test <code>t_test_ind</code></li> <li>Repeated-Measures ANOVA <code>repeated_anova</code></li> <li>Logistic Regression <code>logistic</code></li> <li>Factorial ANOVA (Two-way ANOVA) <code>factorial_anova</code></li> <li>One-Way ANOVA (one_way_anova) <code>one_way_anova</code></li> <li>Pearson Correlation <code>pearson_correlation</code></li> <li>Paired t-test <code>paired_ttest</code></li> <li>Chi-Square Test of Independence <code>chi_square_independence</code></li> <li>Multiple Regression (<code>multiple_regression</code>)</li> <li>Two-Way ANOVA <code>two_way_anova</code></li> <li>Kaplan-Meier Analysis <code>kaplan_meier</code></li> <li>Cox Proportional Hazards <code>cox_ph</code></li> <li>Poisson Regression <code>poisson</code></li> <li>Spearman Correlation <code>spearman</code></li> <li>Wilcoxon Signed-Rank Test <code>wilcoxon_signed_rank</code></li> <li>MANOVA (Multivariate Analysis of Variance) <code>manova</code></li> </ol> <p>More tests to be added in future vesrions.</p>"},{"location":"assumption_sheriff/#key-features","title":"Key Features","text":"<ol> <li>Comprehensive assumption checking</li> <li>Recommendations for alternative methods</li> <li>Flexible integration</li> <li>Commonly used test support</li> </ol> <p>Citation</p> <p>If you use AssumptionSheriff in your research, please cite: <pre><code>@software{assumptionsheriff2024,\n    title = {AssumptionSheriff: A Python Package for Statistical Assumption Checking},\n    author = {Mohsen Askar},\n    e-mail = {ceaser198511@gmail.com},\n    year = {2024},\n    url = {https://pypi.org/project/assumption-sheriff/}\n}\n</code></pre></p>"},{"location":"assumption_sheriff/api/","title":"API Reference","text":""},{"location":"assumption_sheriff/api/#the-package-is-broadly-divided-into","title":"The package is broadly divided into:","text":""},{"location":"assumption_sheriff/api/#mixin-classes","title":"Mixin Classes:","text":"<p>To handle specific assumption checks:</p> <ul> <li><code>NormalityChecker</code> class: Checks normality using Shapiro-Wilk test, skewness, and kurtosis.</li> <li><code>HomoscedasticityChecker</code> class: Checks homoscedasticity using Levene's test.</li> <li><code>MonotonicityChecker</code> class: Checks monotonic relationships using Spearman correlation.</li> <li><code>PairedDataChecker</code> class: Checks if data is properly paired.</li> <li><code>CategoricalDataChecker</code> class: Checks if variables are categorical.</li> <li><code>MultivariateNormalityChecker</code> class: Checks multivariate normality using Mardia's test.</li> <li><code>SurvivalDataChecker</code> class: Checks basic requirements for survival data.</li> <li><code>ProportionalHazardsChecker</code> class: Checks proportional hazards using Schoenfeld residuals.</li> <li><code>PairedDifferenceChecker</code> class: Checks properties of paired differences.</li> </ul>"},{"location":"assumption_sheriff/api/#specific-checkers","title":"Specific Checkers:","text":"<p>Specific checkers for various statistical tests, each inheriting from AssumptionChecker and relevant mixins:</p> <ul> <li><code>IndependentTTestChecker</code> class: For Independent Samples t-test</li> <li><code>RepeatedMeasuresANOVAChecker</code> class: For Repeated ANOVA</li> <li><code>LogisticRegressionChecker</code> class: For Logistic Regression</li> <li><code>PearsonCorrelationChecker</code> class: For Pearson correlation</li> <li><code>PairedTTestChecker</code> class: For Paired t-test</li> <li><code>ChiSquareIndependenceChecker</code> class: For Chi-square independence test</li> <li><code>MultipleRegressionChecker</code> class: For Multiple Linear Regression</li> <li><code>TwoWayANOVAChecker</code> class: For Two-way ANOVA</li> <li><code>KaplanMeierChecker</code> class: For Kaplan-Meier</li> <li><code>CoxPHChecker</code> class: For </li> <li><code>PoissonRegressionChecker</code> class: For Cox Proportional Hazards </li> <li><code>SpearmanCorrelationChecker</code> class: For Spearman Corraletion</li> <li><code>WilcoxonSignedRankChecker</code> class: For Wilcoxon Signed-Rank Test</li> <li><code>MANOVAChecker</code> class: For MANOVA</li> <li><code>OneWayANOVAChecker</code> class: For One-way ANOVA</li> <li><code>FactorialANOVAChecker</code> class: For Factorial ANOVA</li> <li>Main Class: StatisticalTestAssumptions is the main class that manages the assumption checkers. It allows checking assumptions for a specified test type and provides recommendations based on the results.</li> </ul>"},{"location":"assumption_sheriff/api/#inlcuded-tests-assumptions","title":"Inlcuded tests assumptions","text":""},{"location":"assumption_sheriff/api/#independent-samples-t-test-t_test_ind","title":"Independent Samples t-test <code>t_test_ind</code>","text":"<ol> <li>Normality: The dependent variable should be (approximately) normally distributed within each group.</li> <li>Homogeneity of Variances: The population variances in the two groups should be equal (often checked using Levene\u2019s test).</li> <li>Independence of Observations: Each observation in one group is independent of any observation in the other group.</li> </ol>"},{"location":"assumption_sheriff/api/#repeated-measures-anova-repeated_anova","title":"Repeated-Measures ANOVA <code>repeated_anova</code>","text":"<ol> <li>Normality: The dependent variable should be (approximately) normally distributed at each time point or for each repeated measure.</li> <li>Sphericity: The variances of the differences between all possible pairs of repeated-measure conditions should be equal (often checked by Mauchly\u2019s test).</li> <li>Independence of Observations: Observations from different subjects are assumed to be independent, although repeated measures on the same subject are inherently correlated.</li> </ol>"},{"location":"assumption_sheriff/api/#logistic-regression-logistic","title":"Logistic Regression <code>logistic</code>","text":"<ol> <li>Dependent Variable: Binary (two categories, e.g., \u201csuccess/fail\u201d or \u201cdisease/no disease\u201d). Independence of Errors: Residuals (errors) should be independent across observations.</li> <li>Lack of Multicollinearity: Predictor variables should not be too highly correlated with each other.</li> <li>Linearity in the Logit: Although logistic regression is for categorical outcomes, continuous predictors should have a linear relationship with the log odds of the outcome.</li> </ol>"},{"location":"assumption_sheriff/api/#factorial-anova-factorial_anova","title":"Factorial ANOVA <code>factorial_anova</code>","text":"<ol> <li>Normality: The dependent variable in each cell of the design should be (approximately) normally distributed.</li> <li>Homogeneity of Variances: The variance across all cells (factorial combinations of levels) should be equal.</li> <li>Independence of Observations: Observations in each cell are independent of those in other cells and within cells.</li> </ol>"},{"location":"assumption_sheriff/api/#one-way-anova-one_way_anova","title":"One-Way ANOVA <code>one_way_anova</code>","text":"<ol> <li>Normality: The dependent variable should be (approximately) normally distributed in each group.</li> <li>Homogeneity of Variances: The variances in each of the groups are assumed to be equal.</li> <li>Independence of Observations: Observations in one group should be independent from those in other groups.</li> </ol>"},{"location":"assumption_sheriff/api/#pearson-correlation-pearson_correlation","title":"Pearson Correlation <code>pearson_correlation</code>","text":"<ol> <li>Linearity: The relationship between the two variables should be linear.</li> <li>Normality (for Significance Testing): Each variable should be (approximately) normally distributed if you want to use significance tests and confidence intervals for r.</li> <li>Interval or Ratio Scale: Both variables are typically continuous and measured on an interval or ratio scale.</li> <li>Independence of Observations: Each pair of observations comes from independent subjects or units.</li> </ol>"},{"location":"assumption_sheriff/api/#paired-t-test-paired_ttest","title":"Paired t-test <code>paired_ttest</code>","text":"<ol> <li>Normality of Difference Scores: The differences between the paired observations should be (approximately) normally distributed.</li> <li>Dependence of Observations: Each pair is taken from the same subject or matched subjects (hence, \u201cpaired\u201d).</li> <li>No Significant Outliers: Extreme outliers in the difference scores can affect the test.</li> </ol>"},{"location":"assumption_sheriff/api/#chi-square-test-of-independence-chi_square_independence","title":"Chi-Square Test of Independence <code>chi_square_independence</code>","text":"<ol> <li>Independence of Observations: Each subject or unit should be counted only once in the contingency table.</li> <li>Expected Cell Frequency: Each cell in the contingency table should have an expected count of at least 5 (rule of thumb for validity of p-values).</li> <li>Categorical Variables: Both variables should be categorical (nominal or ordinal).</li> </ol>"},{"location":"assumption_sheriff/api/#multiple-regression-multiple_regression","title":"Multiple Regression <code>multiple_regression</code>","text":"<ol> <li>Linearity: The relationship between each predictor and the outcome (dependent variable) is assumed to be linear in the parameters.</li> <li>Independence of Errors: Residuals should be independent (often checked by plotting residuals vs. predicted values).</li> <li>Homoscedasticity: The variance of residuals is constant across all levels of the predictors (also checked by residual plots).</li> <li>Normality of Residuals: The residuals should be (approximately) normally distributed (checked with Q-Q plots).</li> <li>Lack of Multicollinearity: Predictors should not be too highly correlated with each other.</li> </ol>"},{"location":"assumption_sheriff/api/#two-way-anova-two_way_anova","title":"Two-Way ANOVA <code>two_way_anova</code>","text":"<ol> <li>Normality: The dependent variable in each group (combination of two independent factors) should be (approximately) normally distributed.</li> <li>Homogeneity of Variances: The variances across all factor-level combinations should be equal (Levene\u2019s test is common).</li> <li>Independence of Observations: Observations in one factor-level combination are independent from other factor-level combinations and within each combination.</li> </ol>"},{"location":"assumption_sheriff/api/#kaplan-meier-analysis-kaplan_meier","title":"Kaplan-Meier Analysis <code>kaplan_meier</code>","text":"<ol> <li>Random Censoring: Assumes that censoring is non-informative (the reason an individual leaves the study or is censored is independent of their underlying risk).</li> <li>Independence of Survival Times: Each subject\u2019s survival time is independent of others.</li> <li>Time-to-Event Data: Typically used when the outcome is the time until an event (e.g., death, relapse).</li> </ol>"},{"location":"assumption_sheriff/api/#cox-proportional-hazards-model-cox_ph","title":"Cox Proportional Hazards Model <code>cox_ph</code>","text":"<ol> <li>Proportional Hazards: The hazard functions for different groups (or at different levels of a covariate) are proportional over time (i.e., hazard ratios remain constant over time).</li> <li>Random/Non-Informative Censoring: Similar to Kaplan-Meier, censoring should not be related to the outcome.</li> <li>Linearity (for Continuous Covariates): Often assumed that continuous covariates have a log-linear relationship with the hazard.</li> <li>Independence of Observations: Each subject\u2019s time-to-event is independent (unless modeling random effects or frailty for clustering).</li> </ol>"},{"location":"assumption_sheriff/api/#poisson-regression-poisson","title":"Poisson Regression <code>poisson</code>","text":"<ol> <li>Count Outcome Variable: The dependent variable is a count (e.g., number of doctor visits).</li> <li>Mean-Variance Relationship: The Poisson model assumes the mean and variance are equal. (If variance &gt; mean significantly, a Negative Binomial model might be preferred.)</li> <li>Independence of Observations: Each count is assumed to be independent of the others.</li> <li>Linearity in the Log Link: The log of the expected count is assumed to be a linear combination of predictors.</li> </ol>"},{"location":"assumption_sheriff/api/#spearman-correlation-spearman","title":"Spearman Correlation <code>spearman</code>","text":"<ol> <li>Monotonic Relationship: The relationship between the two variables should be monotonic (does not have to be linear).</li> <li>Ordinal or Interval/Ratio Data: Although often used for ordinal data, Spearman correlation can also handle interval/ratio data that fail Pearson\u2019s normality assumption.</li> <li>Independence of Observations: Each pair of observations is assumed independent.</li> </ol>"},{"location":"assumption_sheriff/api/#wilcoxon-signed-rank-test-wilcoxon_signed_rank","title":"Wilcoxon Signed-Rank Test <code>wilcoxon_signed_rank</code>","text":"<ol> <li>Paired or Matched Samples: The same subjects measured twice, or matched subjects.</li> <li>Ordinal or Continuous Data: Used when data are ordinal or not normally distributed, but we assume differences can be meaningfully ranked.</li> <li>Symmetry of Distribution of Differences (Ideal): While not as strict as the normality assumption, it is often assumed that the distribution of differences is symmetrical around the median.</li> </ol>"},{"location":"assumption_sheriff/api/#manova-multivariate-analysis-of-variance-manova","title":"MANOVA (Multivariate Analysis of Variance) <code>manova</code>","text":"<ol> <li>Multivariate Normality: The combination of dependent variables follows a multivariate normal distribution within each group.</li> <li>Homogeneity of Variance-Covariance Matrices: The variance-covariance matrices for the dependent variables are the same in each group (Box\u2019s M test).</li> <li>Independence of Observations: Observations across groups (and within groups) are independent.</li> <li>No Multicollinearity Among Dependent Variables: If the dependent variables are very highly correlated, MANOVA might not be the best approach.</li> </ol>"},{"location":"assumption_sheriff/api/#suggested-test-alternatives-explanation","title":"Suggested test alternatives explanation","text":""},{"location":"assumption_sheriff/api/#independent-t-test","title":"Independent T-Test:","text":"<ul> <li> <p>Suggested Alternatives: Mann-Whitney U test, Welch's t-test</p> </li> <li> <p>Explanation:</p> </li> <li> <p>Mann-Whitney U test is a non-parametric alternative that does not assume normality.</p> </li> <li> <p>Welch's t-test is used when the assumption of equal variances is violated.</p> </li> </ul>"},{"location":"assumption_sheriff/api/#repeated-measures-anova","title":"Repeated Measures ANOVA:","text":"<ul> <li> <p>Suggested Alternatives: Friedman test, Mixed-effects model</p> </li> <li> <p>Explanation:</p> <ul> <li> <p>Friedman test is a non-parametric alternative for repeated measures.</p> </li> <li> <p>Mixed-effects model can handle violations of sphericity and other complex data structures.</p> </li> </ul> </li> </ul>"},{"location":"assumption_sheriff/api/#logistic-regression","title":"Logistic Regression:","text":"<ul> <li> <p>Suggested Alternatives: Penalized regression (Ridge, Lasso), Decision trees</p> </li> <li> <p>Explanation:</p> <ul> <li> <p>Penalized regression methods like Ridge and Lasso can handle multicollinearity.</p> </li> <li> <p>Decision trees do not assume linearity or independence of errors.</p> </li> </ul> </li> </ul>"},{"location":"assumption_sheriff/api/#pearson-correlation","title":"Pearson Correlation:","text":"<ul> <li> <p>Suggested Alternatives: Spearman rank correlation, Kendall rank correlation, Robust correlation methods</p> </li> <li> <p>Explanation:</p> <ul> <li> <p>Spearman rank correlation and Kendall rank correlation are non-parametric and do not assume normality.</p> </li> <li> <p>Robust correlation methods can handle outliers.</p> </li> </ul> </li> </ul>"},{"location":"assumption_sheriff/api/#paired-t-test","title":"Paired T-Test:","text":"<ul> <li> <p>Suggested Alternatives: Wilcoxon signed-rank test, Sign test, Randomization test</p> </li> <li> <p>Explanation:</p> <ul> <li> <p>Wilcoxon signed-rank test is a non-parametric alternative for paired data.</p> </li> <li> <p>Sign test is another non-parametric alternative.</p> </li> <li> <p>Randomization test can be used when assumptions are severely violated.</p> </li> </ul> </li> </ul>"},{"location":"assumption_sheriff/api/#chi-square-test-of-independence","title":"Chi-Square Test of Independence:","text":"<ul> <li> <p>Suggested Alternatives: Fisher's exact test, G-test of independence, Freeman-Halton test, Log-linear analysis</p> </li> <li> <p>Explanation:</p> <ul> <li> <p>Fisher's exact test is used for small sample sizes.</p> </li> <li> <p>G-test is an alternative to the chi-square test.</p> </li> <li> <p>Freeman-Halton test extends Fisher's test to larger tables.</p> </li> <li> <p>Log-linear analysis is used for more complex categorical data.</p> </li> </ul> </li> </ul>"},{"location":"assumption_sheriff/api/#multiple-linear-regression","title":"Multiple Linear Regression:","text":"<ul> <li> <p>Suggested Alternatives: Ridge Regression, Lasso Regression, Robust Regression, Quantile Regression, Non-linear regression models</p> </li> <li> <p>Explanation:</p> <ul> <li> <p>Ridge and Lasso Regression address multicollinearity.</p> </li> <li> <p>Robust Regression handles outliers.</p> </li> <li> <p>Quantile Regression does not assume homoscedasticity.</p> </li> <li> <p>Non-linear regression models are used when linearity is violated.</p> </li> </ul> </li> </ul>"},{"location":"assumption_sheriff/api/#two-way-anova","title":"Two-Way ANOVA:","text":"<ul> <li> <p>Suggested Alternatives: Non-parametric factorial analysis, Robust two-way ANOVA, Aligned Rank Transform ANOVA, Separate non-parametric tests with correction, Mixed-effects model</p> </li> <li> <p>Explanation:</p> <ul> <li> <p>Non-parametric factorial analysis is used when assumptions are violated.</p> </li> <li> <p>Robust ANOVA methods handle violations of assumptions.</p> </li> <li> <p>Aligned Rank Transform ANOVA is a non-parametric alternative.</p> </li> <li> <p>Mixed-effects model can handle complex designs.</p> </li> </ul> </li> </ul>"},{"location":"assumption_sheriff/api/#kaplan-meier-survival-analysis","title":"Kaplan-Meier Survival Analysis:","text":"<ul> <li> <p>Suggested Alternatives: Cox Proportional Hazards model, Parametric survival models, Competing risks analysis, Time-varying coefficient models</p> </li> <li> <p>Explanation:</p> <ul> <li> <p>Cox Proportional Hazards model is more flexible.</p> </li> <li> <p>Parametric survival models assume specific distributions.</p> </li> <li> <p>Competing risks analysis is used when there are competing events.</p> </li> <li> <p>Time-varying coefficient models handle time-dependent covariates.</p> </li> </ul> </li> </ul>"},{"location":"assumption_sheriff/api/#cox-proportional-hazards-regression","title":"Cox Proportional Hazards Regression:","text":"<ul> <li> <p>Suggested Alternatives: Stratified Cox model, Time-varying coefficient Cox model, Parametric survival models, Additive hazards models</p> </li> <li> <p>Explanation:</p> <ul> <li> <p>Stratified Cox model handles non-proportional hazards.</p> </li> <li> <p>Time-varying coefficient models address time-dependent effects.</p> </li> <li> <p>Parametric survival models assume specific distributions.</p> </li> <li> <p>Additive hazards models are an alternative to proportional hazards.</p> </li> </ul> </li> </ul>"},{"location":"assumption_sheriff/api/#poisson-regression-continued","title":"Poisson Regression (continued):","text":"<ul> <li> <p>Suggested Alternatives: Negative Binomial Regression, Zero-inflated Poisson Regression, Zero-inflated Negative Binomial Regression, Quasi-Poisson Regression</p> </li> <li> <p>Explanation:</p> <ul> <li> <p>Negative Binomial Regression is suitable when there is overdispersion (variance greater than the mean) in count data.</p> </li> <li> <p>Zero-inflated Poisson Regression is used when there are more zeros in the data than expected under a standard Poisson model.</p> </li> <li> <p>Zero-inflated Negative Binomial Regression combines the handling of excess zeros and overdispersion.</p> </li> <li> <p>Quasi-Poisson Regression is another approach to handle overdispersion by adjusting the variance function.</p> </li> </ul> </li> </ul>"},{"location":"assumption_sheriff/api/#spearmans-rank-correlation","title":"Spearman's Rank Correlation:","text":"<ul> <li> <p>Suggested Alternatives: Kendall's tau, Kendall's tau-b (for ties), Pearson correlation (if relationship is linear), Distance correlation (for non-monotonic relationships)</p> </li> <li> <p>Explanation:</p> <ul> <li> <p>Kendall's tau is a non-parametric measure of correlation that is less sensitive to ties than Spearman's.</p> </li> <li> <p>Kendall's tau-b is specifically designed to handle ties in the data.</p> </li> <li> <p>Pearson correlation can be used if the relationship is linear and assumptions of normality are met.</p> </li> <li> <p>Distance correlation is a more general measure that can detect both linear and non-linear associations.</p> </li> </ul> </li> </ul>"},{"location":"assumption_sheriff/api/#wilcoxon-signed-rank-test","title":"Wilcoxon Signed-Rank Test:","text":"<ul> <li> <p>Suggested Alternatives: Sign test (for asymmetric differences), Paired t-test (if differences are normal), Permutation test, Bootstrap methods</p> </li> <li> <p>Explanation:</p> <ul> <li> <p>Sign test is a simpler non-parametric test that can be used if the differences are not symmetrically distributed.</p> </li> <li> <p>Paired t-test is appropriate if the differences are normally distributed.</p> </li> <li> <p>Permutation test is a non-parametric method that does not rely on distributional assumptions.</p> </li> </ul> <p>Bootstrap methods provide a flexible approach to estimate the sampling distribution of the test statistic.</p> </li> </ul>"},{"location":"assumption_sheriff/api/#manova-multivariate-analysis-of-variance","title":"MANOVA (Multivariate Analysis of Variance):","text":"<ul> <li> <p>Suggested Alternatives: Separate univariate ANOVAs with Bonferroni correction, Robust MANOVA, Permutation MANOVA, Non-parametric multivariate tests (e.g., NPMANOVA), Linear Discriminant Analysis</p> </li> <li> <p>Explanation:</p> <ul> <li> <p>Separate univariate ANOVAs with Bonferroni correction control for Type I error across multiple tests.</p> </li> <li> <p>Robust MANOVA methods handle violations of assumptions such as multivariate normality.</p> </li> <li> <p>Permutation MANOVA is a non-parametric alternative that does not assume normality.</p> </li> <li> <p>Non-parametric multivariate tests like NPMANOVA are used when assumptions are violated.</p> </li> <li> <p>Linear Discriminant Analysis can be used for classification purposes when MANOVA assumptions are not met.</p> </li> </ul> </li> </ul>"},{"location":"assumption_sheriff/api/#one-way-anova","title":"One-Way ANOVA:","text":"<ul> <li> <p>Suggested Alternatives: Kruskal-Wallis H-test, Welch's ANOVA, Brown-Forsythe test</p> </li> <li> <p>Explanation:</p> <ul> <li> <p>Kruskal-Wallis H-test is a non-parametric alternative that does not assume normality.</p> </li> <li> <p>Welch's ANOVA is used when the assumption of equal variances is violated.</p> </li> <li> <p>Brown-Forsythe test is another alternative for testing equality of means when variances are unequal.</p> </li> </ul> </li> </ul>"},{"location":"assumption_sheriff/api/#factorial-anova","title":"Factorial ANOVA:","text":"<ul> <li> <p>Suggested Alternatives: Non-parametric factorial analysis, Mixed-effects model, Robust ANOVA</p> </li> <li> <p>Explanation:</p> <ul> <li> <p>Non-parametric factorial analysis is used when assumptions of normality and homoscedasticity are violated.</p> </li> <li> <p>Mixed-effects model can handle complex designs and violations of sphericity.</p> </li> </ul> </li> </ul>"},{"location":"assumption_sheriff/usage/","title":"Usage Guide","text":""},{"location":"assumption_sheriff/usage/#installation","title":"Installation","text":"<p>The package can be installed using <code>pip</code>. The dependdencies are <code>pandas</code>,<code>numpy</code>, <code>scipy</code>, <code>lifelines</code>, and <code>statsmodels</code>.</p> <pre><code>pip install assumption_sheriff\n</code></pre>"},{"location":"assumption_sheriff/usage/#quick-start","title":"Quick Start","text":"<pre><code># Basic import of everything\nimport assumption_sheriff as ash\n</code></pre> <pre><code># or direct import of specific components\nfrom assumption_sheriff import StatisticalTestAssumptions\n</code></pre>"},{"location":"assumption_sheriff/usage/#detailed-user-example","title":"Detailed user example","text":"<pre><code># Generate a sample data to test the package \nimport numpy as np\nimport pandas as pd\n\nnp.random.seed(123)\nn = 100\n\n# 1. Independent groups for t-test / one-way ANOVA / two-way ANOVA (factor A/B)\ngroup_bin = np.random.choice([0, 1], size=n)  \nfactorA = np.random.choice(['A1','A2'], size=n)  \nfactorB = np.random.choice(['B1','B2'], size=n) \n\n# 2. Continuous variables (for t-tests, ANOVAs, correlations, regressions, etc.)\ncont_var1 = np.random.normal(loc=50, scale=10, size=n)   \ncont_var2 = np.random.normal(loc=0, scale=5, size=n)     \ncont_var3 = np.random.normal(loc=100, scale=20, size=n) \n\n# 3. Repeated-measures variables (for repeated-measures ANOVA)\nrm_time1 = np.random.normal(loc=5, scale=1, size=n)  \nrm_time2 = rm_time1 + np.random.normal(loc=0.5, scale=0.5, size=n)  \nrm_time3 = rm_time1 + np.random.normal(loc=1.0, scale=0.5, size=n)  \n\n# 4. Paired data (for paired t-tests or Wilcoxon signed-rank)\npaired_pre = np.random.normal(loc=10, scale=2, size=n)\npaired_post = paired_pre + np.random.normal(loc=-1, scale=1, size=n)\n\n# 5. Logistic outcome (binary) for logistic regression\nlogistic_outcome = np.random.binomial(n=1, p=0.4, size=n)\n\n# 6. Categorical variables (for Chi-Square)\ncat_var1 = np.random.choice(['Yes','No'], size=n)\ncat_var2 = np.random.choice(['High','Low'], size=n)\n\n# 7. Survival data (time-to-event + event indicator for KM/Cox)\ntime_to_event = np.random.exponential(scale=10, size=n)\nevent_occurred = np.random.binomial(n=1, p=0.7, size=n)\n\n# 8. Count data (for Poisson regression)\ncount_data = np.random.poisson(lam=2, size=n)\n\n# 9. Ordinal data (for Spearman correlation or ordinal logistic)\nordinal_data = np.random.choice(['Mild','Moderate','Severe'], size=n)\n\n# 10. Additional continuous variables for correlations / MANOVA\nmanova_var1 = np.random.normal(loc=30, scale=5, size=n)\nmanova_var2 = np.random.normal(loc=60, scale=10, size=n)\n\n# Assemble everything into a DataFrame\ndata = pd.DataFrame({\n    'group_bin': group_bin,\n    'factorA': factorA,\n    'factorB': factorB,\n    'cont_var1': cont_var1,\n    'cont_var2': cont_var2,\n    'cont_var3': cont_var3,\n    'rm_time1': rm_time1,\n    'rm_time2': rm_time2,\n    'rm_time3': rm_time3,\n    'paired_pre': paired_pre,\n    'paired_post': paired_post,\n    'logistic_outcome': logistic_outcome,\n    'cat_var1': cat_var1,\n    'cat_var2': cat_var2,\n    'time_to_event': time_to_event,\n    'event_occurred': event_occurred,\n    'count_data': count_data,\n    'ordinal_data': ordinal_data,\n    'manova_var1': manova_var1,\n    'manova_var2': manova_var2\n})\n\nprint(data.head(5))\n</code></pre> <pre><code>   group_bin factorA factorB  cont_var1  cont_var2   cont_var3  rm_time1  \\\n0          0      A2      B2  54.743473  -6.186766   96.147700  4.413384   \n1          1      A1      B2  44.360761   0.620279  108.982712  5.154290   \n2          0      A1      B1  40.026785  -8.002203   97.092729  3.852763   \n3          0      A2      B1  38.999569   3.769344  137.374529  6.520166   \n4          0      A2      B1  42.435628  -1.234079   89.625923  5.189043\n\n   rm_time2  rm_time3  paired_pre  paired_post  logistic_outcome cat_var1  \\\n0  5.256786  5.338739   10.545469     8.320940                 0       No   \n1  5.947174  7.147672   10.850672    10.859323                 0      Yes   \n2  4.585648  4.515028    9.538192     8.055827                 1       No   \n3  6.311219  7.372252   17.143158    17.222955                 0      Yes   \n4  5.909335  5.162844    9.207688     7.786610                 1       No\n\n  cat_var2  time_to_event  event_occurred  count_data ordinal_data  \\\n0      Low       0.491169               1           5       Severe   \n1      Low      21.322712               1           2       Severe   \n2      Low       6.101167               1           0       Severe   \n3      Low      17.788497               1           1     Moderate   \n4     High       3.188856               0           1         Mild\n\n   manova_var1  manova_var2  \n0    35.227235    49.717239  \n1    25.034866    52.544302  \n2    32.711234    56.899888  \n3    32.427786    48.581779  \n4    33.709235    69.263657\n</code></pre>"},{"location":"assumption_sheriff/usage/#t-tests","title":"T-tests","text":"<pre><code># for independent t-test\n# ---------------------\n# Initialize checker\nchecker = ash.StatisticalTestAssumptions()\n\n# Check assumptions for independent t-test\nresults = checker.check_assumptions(\n    data=data,\n    test_type='t_test_ind',\n    variables=['cont_var1', 'cont_var2'],\n    group_column='group_bin'\n)\n\n# Get recommendation\nrecommendation = checker.get_recommendation(results)\nprint(recommendation)\n</code></pre> <pre><code>\u2713 All assumptions are met. You can proceed with the Independent t-test.\n</code></pre> <pre><code># for paired t-test\n# ---------------------\nchecker = ash.StatisticalTestAssumptions()\nresults = checker.check_assumptions(\n    data=data,\n    test_type='paired_ttest',\n    variables=['paired_pre', 'paired_post']\n)\n# Get recommendations\nrecommendation = checker.get_recommendation(results)\nprint(recommendation)\n</code></pre> <pre><code>\u2713 All assumptions are met. You can proceed with the Paired t-test.\n</code></pre>"},{"location":"assumption_sheriff/usage/#anovas","title":"ANOVAs","text":"<pre><code># One-way ANOVA\n# -------------\nchecker = ash.StatisticalTestAssumptions()\nresults = checker.check_assumptions(\n    data=data,\n    test_type='one_way_anova',\n    variables=['cont_var1', 'cont_var2'],\n    group_column='group_bin'\n)\n\nrecommendation = checker.get_recommendation(results)\nprint(recommendation)\n</code></pre> <pre><code>\u2713 All assumptions are met. You can proceed with the One-way ANOVA.\n</code></pre> <pre><code># Two-Way ANOVA\n# ----------------\nchecker = ash.StatisticalTestAssumptions()\nresults = checker.check_assumptions(\n    data=data,\n    test_type='two_way_anova',\n    variables=['cont_var1'],\n    #dependent_var='cont_var1',\n    factors=['factorA', 'factorA']\n)\n\nrecommendation = checker.get_recommendation(results)\nprint(recommendation) \n</code></pre> <pre><code>\u2713 All assumptions are met. You can proceed with the Two-way ANOVA.\n</code></pre> <pre><code># Factorial (Two-way) ANOVA\n# -----------------------\nchecker = ash.StatisticalTestAssumptions()\n# Sample data structure\ndata2 = pd.DataFrame({\n    'fertilizer': ['A', 'A', 'B', 'B'] * 25,\n    'watering': ['daily', 'weekly'] * 50,\n    'yield': np.random.normal(loc=[50, 45, 60, 55] * 25, scale=5)\n})\n\nresults = checker.check_assumptions(\n    data=data2,\n    test_type='factorial_anova',\n    variables=['yield'],\n    group_columns= ['fertilizer', 'watering']\n)\n\nrecommendation = checker.get_recommendation(results)\nprint(recommendation)\n</code></pre> <pre><code>\u26a0 Some assumptions for Factorial ANOVA are violated:\n\n- Insufficient sample size in some cells (minimum 25 &lt; required 30)\n\nConsider these alternatives:\n- Non-parametric factorial analysis\n- Mixed-effects model\n- Robust ANOVA\n</code></pre> <pre><code># Repeated measures ANOVA\n#-------------------------\nchecker = ash.StatisticalTestAssumptions()\n# Check assumptions\nresults = checker.check_assumptions(\n    data=data,\n    test_type='repeated_anova',\n    variables=['rm_time1', 'rm_time2', 'rm_time3'],\n    subject_column='group_bin'\n)\n\nrecommendation = checker.get_recommendation(results)\nprint(recommendation)\n</code></pre> <pre><code>\u2713 All assumptions are met. You can proceed with the Repeated Measures ANOVA.\n</code></pre> <pre><code># for MANOVA (Multivariate Analysis of Variance)\n#---------------------------------------------------\n\nchecker = ash.StatisticalTestAssumptions()\nresults = checker.check_assumptions(\n    data=data,\n    test_type='manova',\n    variables=['manova_var1', 'manova_var2'],\n    group_col='group_bin'\n)\n\nrecommendation = checker.get_recommendation(results)\nprint(recommendation)\n</code></pre> <pre><code>\u26a0 Some assumptions for MANOVA are violated:\n\n- Multivariate normality violated in group_1\n- Number of dependent variables should ideally be greater than number of groups\n\nConsider these alternatives:\n- Separate univariate ANOVAs with Bonferroni correction\n- Robust MANOVA\n- Permutation MANOVA\n- Non-parametric multivariate tests (e.g., NPMANOVA)\n- Linear Discriminant Analysis\n</code></pre>"},{"location":"assumption_sheriff/usage/#correlation-tests","title":"Correlation tests","text":"<pre><code># for Pearson corraltion \n# --------------------------\nchecker = ash.StatisticalTestAssumptions()\nresults = checker.check_assumptions(\n    data=data,\n    test_type='pearson_correlation',\n    variables=['cont_var1', 'cont_var2']\n)\n\nrecommendation = checker.get_recommendation(results)\nprint(recommendation)\n</code></pre> <pre><code>\u26a0 Some assumptions for Pearson Correlation are violated:\n\n- Variable pair cont_var1_vs_cont_var2 may not have a monotonic relationship (Spearman correlation=0.07)\n\nConsider these alternatives:\n- Spearman rank correlation\n- Kendall rank correlation\n- Robust correlation methods\n</code></pre> <pre><code># for spearman correlation\n#----------------------------\n\nchecker = ash.StatisticalTestAssumptions()\nresults = checker.check_assumptions(\n    data=data,\n    test_type='spearman',\n    variables=['ordinal_data', 'cont_var2']\n)\n\nrecommendation = checker.get_recommendation(results)\nprint(recommendation) \n</code></pre> <pre><code>\u2713 All assumptions are met. You can proceed with the Spearman's Rank Correlation.\n</code></pre>"},{"location":"assumption_sheriff/usage/#chi-square-independence","title":"Chi-square independence","text":"<pre><code># for Chi-square test \n# ---------------------\nchecker = ash.StatisticalTestAssumptions()\nresults = checker.check_assumptions(\n    data=data,\n    test_type='chi_square_independence',\n    variables=['cat_var1', 'cat_var1']\n)\n\nrecommendation = checker.get_recommendation(results)\nprint(recommendation)\n</code></pre> <pre><code>\u2713 All assumptions are met. You can proceed with the Chi-square test of independence.\n</code></pre>"},{"location":"assumption_sheriff/usage/#regression","title":"Regression","text":"<pre><code># For Logistic Regression\n#------------------------\nchecker = ash.StatisticalTestAssumptions()\n# Check assumptions\nresults = checker.check_assumptions(\n    data=data,\n    test_type='logistic',\n    variables=['cont_var1', 'cont_var2', 'cont_var3'],\n    dependent_var='logistic_outcome'\n)\n\nrecommendation = checker.get_recommendation(results)\nprint(recommendation)\n</code></pre> <pre><code>\u26a0 Some assumptions for Logistic Regression are violated:\n\n- High multicollinearity detected for 'const' (VIF=58.70)\n\nConsider these alternatives:\n- Penalized regression (Ridge, Lasso)\n- Decision trees\n</code></pre> <pre><code># Multiple Linear Regression \n# ----------------------------\nchecker = StatisticalTestAssumptions()\nresults = checker.check_assumptions(\n    data=data,\n    test_type='multiple_regression',\n    variables=['cont_var1', 'cont_var2', 'cont_var3'],\n    dependent_var='logistic_outcome'\n)\n\nrecommendation = checker.get_recommendation(results)\nprint(recommendation)\n</code></pre> <pre><code>\u26a0 Some assumptions for Multiple Linear Regression are violated:\n\n- Residuals are not normally distributed (Shapiro-Wilk p=0.0000)\n- Non-linear relationship detected for predictor 'cont_var1'\n- Non-linear relationship detected for predictor 'cont_var2'\n- High multicollinearity detected for variables: ['const']\n</code></pre> <pre><code># for Poisson regression \n#-----------------------------------------\nchecker = ash.StatisticalTestAssumptions()\nresults = checker.check_assumptions(\n    data=data,\n    test_type='poisson',\n    variables=[\n        'count_data',    # dependent variable must be first\n        'cont_var1',     # predictors follow\n        'cont_var2'\n    ],\n    offset_var='exposure_time'  # Optional\n)\n\nrecommendation = checker.get_recommendation(results)\nprint(recommendation)\n</code></pre> <pre><code>\u26a0 Some assumptions for Poisson Regression are violated:\n\n- High multicollinearity detected for variables: ['const']\n\nConsider these alternatives:\n- Negative Binomial Regression\n- Zero-inflated Poisson Regression\n- Zero-inflated Negative Binomial Regression\n- Quasi-Poisson Regression\n</code></pre>"},{"location":"assumption_sheriff/usage/#survival-analysis","title":"Survival analysis","text":"<pre><code># for Kaplan-Meier \n# ----------------\nchecker = ash.StatisticalTestAssumptions()\nresults = checker.check_assumptions(\n    data=data,\n    test_type='kaplan_meier',\n    variables=['time_to_event', 'event_occurred'],  # time variable first, event variable second\n    group_col='group_bin'  # optional grouping variable\n)\n\nrecommendation = checker.get_recommendation(results)\nprint(recommendation) \n</code></pre> <pre><code>\u2713 All assumptions are met. You can proceed with the Kaplan-Meier survival analysis.\n</code></pre> <pre><code># for Cox Proportional Hazards \n# -----------------------------\nchecker = ash.StatisticalTestAssumptions()\n\ncox_results = checker.check_assumptions(\n    data=data,\n    test_type='cox_ph',\n    variables=['time_to_event', 'event_occurred'],  # time variable first, event variable second\n    group_col='group_bin' \n) \n\nrecommendation = checker.get_recommendation(cox_results)\nprint(recommendation) \n</code></pre> <pre><code>\u2713 All assumptions are met. You can proceed with the Cox Proportional Hazards Regression.\n</code></pre>"},{"location":"assumption_sheriff/usage/#non-parametric-tests","title":"Non-parametric tests","text":"<pre><code># for Wilcoxon Signed-Rank Test\n#-------------------------------\nchecker = ash.StatisticalTestAssumptions()\nresults = checker.check_assumptions(\n    data=data,\n    test_type='wilcoxon_signed_rank',\n    variables=['paired_pre', 'paired_post']\n)\n\nrecommendation = checker.get_recommendation(results)\nprint(recommendation)\n</code></pre> <pre><code>\u2713 All assumptions are met. You can proceed with the Wilcoxon Signed-Rank Test.\n</code></pre>"},{"location":"assumption_sheriff/usage/#common-issues-and-solutions","title":"Common issues and solutions","text":""},{"location":"assumption_sheriff/usage/#1-handling-missing-data","title":"1. Handling missing data","text":"<p><code>AssumptionSheriff</code> automatically handles missing data in most cases. However, for best results: - Remove or impute missing values before checking assumptions - Ensure complete cases for paired tests - Document any data preprocessing steps</p>"},{"location":"assumption_sheriff/usage/#2-dealing-with-outliers","title":"2. Dealing with outliers","text":"<p>When outliers are detected: - Review them for data entry errors - Consider robust statistical methods - Document justification for outlier handling</p>"},{"location":"assumption_sheriff/usage/#3-small-sample-sizes","title":"3. Small sample sizes","text":"<p>For small samples: - Consider non-parametric alternatives - Use exact tests when available - Be cautious with assumption violations</p>"},{"location":"audio2topics/","title":"Overview","text":""},{"location":"audio2topics/#audio2topics-package","title":"<code>Audio2Topics</code> Package \ud83d\udd0a\ud83d\uddc2\ufe0f","text":"<p><code>Audio2Topics</code> is python packge that automizes topic extraction from voice files. The package is originally designed to aid researchers that performs interview research. Interview research typically incorporates a series of steps, starting from planning research questions, performing interviews, transcribing, and thorough manual text analysis to extract the main themes and topics of the transcribed text. This manual analysis phase is usually long and time-consuming. Additionally, the feasibility of manual analysis is limited by the volume of transcribed data.  <code>Audio2Topics</code> accelerating this step by automtically converting voice files of interviews into topics simply and effectively.</p> <p>Coming soon</p>"},{"location":"audio2topics/api/","title":"API Reference","text":"<p>Coming soon! :)</p>"},{"location":"audio2topics/usage/","title":"Usage Guide","text":"<p>Coming soon! :)</p>"},{"location":"data_insightron/","title":"Overview","text":""},{"location":"data_insightron/#datainsightron-package","title":"<code>DataInsightron</code> Package \ud83e\udd16","text":"<p><code>DataInsightron</code>\ud83e\udd16 is a Python package designed to provide a comprehensive overview of a dataset characteristics. It computes a variety of metrics\u2014ranging from basic dimensions, Features related metrics, Quality metrics, possible best-performing ML algorithm in prediction tasks, Computational resources requirements, and Complexity measure. <code>DataInsightron</code> is desigend to guide you fully undertand your dataset as a whole. The package is particularly helpful in the exploratory data analysis (EDA) phase, as it automates the extraction of numerous data descriptors that would otherwise be computed through multiple manual steps.</p> <p>Coming soon! :)</p>"},{"location":"data_insightron/api/","title":"<code>DataInsightron</code> \ud83e\udd16","text":"<p>Coming soon! :)</p>"},{"location":"data_insightron/usage/","title":"Usage Guide","text":"<p>Coming soon! :) </p>"},{"location":"modularity_encoding/","title":"Overview","text":""},{"location":"modularity_encoding/#modularity-encoding-package","title":"<code>Modularity Encoding</code> Package \ud83d\udee0\ufe0f\ud83d\udce6","text":"<p>The <code>modularity_encoding</code> package provides functionality for grouping high dimensional Health Coding Systems (HCSs). In the medical domain, these code systems can be challenging to handle in big datasets while implementing machine learning model for classification or prediction purposes. Examples of these code systems are International Classification of Diseases (ICD) codes, Anatomical Therapeutic Chemical (ATC) codes, Diseases Related Group (DRG) codes, etc. These code systems include thousands of codes and binarizing them in many columns will greatly increase the sparsity of the dataset leading to worse model performances. More specifically, the package includes functions for creating a network of code systems on the co-occurrences of the code systems codes in the dataset population, then, detecting communities(modules) within the network. These modules are a way to group the codes in a clinically-relevant and data-driven way. After that, the package provide a new column which assigns the code system codes to their corresponding community modules. By this way, the model dimensions will be significantly reduced  form thousands to a handful number of dimensions.</p>"},{"location":"modularity_encoding/#key-features","title":"Key Features","text":"<ul> <li>Network Creation: The package creates a network from the HCSs codes based on their co-occurrences within a dataset.</li> <li>Community Detection: It identifies communities (or modules) within the network. These communities group the HCSs codes in a way that is both clinically relevant and data-driven, enhancing the interpretability and utility of the codes.</li> <li>Dimensionality Reduction: By assigning codes to their respective community modules, <code>modularity_encoding</code> significantly reduces the dimensionality of the data\u2014from potentially thousands of columns to just a few. This reduction helps in managing dataset sparsity and improving model performance.</li> </ul>"},{"location":"modularity_encoding/#license","title":"License","text":"<p>Released under the MIT License: Copyright (C) 2024 modularity_encoding</p> <p>Developed by: Mohsen Askar ceaser198511@gmail.com</p>"},{"location":"modularity_encoding/#citation","title":"Citation","text":"<p>If you use <code>modularity_encoding</code> in your research, we would appreciate a citation to this paper:</p> <p>\u201cUsing Network Analysis Modularity to Group Health Code Systems and Decrease Dimensionality in Machine Learning Models\u201d https://doi.org/10.1016/j.rcsop.2024.100463</p>"},{"location":"modularity_encoding/api/","title":"API Reference","text":""},{"location":"modularity_encoding/api/#functions-overview","title":"Functions Overview","text":"<p>The <code>modularity_encoding</code> package comprises One main function and five supplementary functions. These functions can either be used together in a pipeline for detailed control and inspection or executed all at once using the main function for convenience.</p>"},{"location":"modularity_encoding/api/#main-function","title":"Main Function","text":"<ul> <li><code>modularity_encode</code>: Executes the full modularity encoding workflow in a single call. This function is ideal for users who need a straightforward and quick way to apply the encoding to their dataset.</li> </ul>"},{"location":"modularity_encoding/api/#supplementary-functions","title":"Supplementary Functions","text":"<p>These functions are designed for users who prefer to manually handle each step of the process, allowing for greater flexibility and inspection at each stage:</p> <ul> <li><code>create_code_system_network</code>: Constructs a network from the health coding system codes based on their occurrences within a dataset.</li> <li><code>detect_code_system_communities</code>: Identifies communities (or modules) within the created network.</li> <li><code>encode_code_system_to_module</code>: Assigns each code in the dataset to the detected community module, effectively reducing the dimensionality of the dataset.</li> <li><code>print_edge_list</code>: Outputs a list of edges from the network, useful for debugging or detailed analysis.</li> <li><code>assign_module</code>: Assigns and records the module for each code system code in the dataset.</li> </ul>"},{"location":"modularity_encoding/api/#the-main-function-modularity_encode","title":"The main function <code>modularity_encode</code>","text":"<p>The <code>modularity_encode</code> function is used to execute the whole flow in one line of code. It takes in a pandas data frame data that contains patient data with a code_system column.</p>"},{"location":"modularity_encoding/api/#parameters","title":"Parameters:","text":"<ul> <li>data (<code>pd.DataFrame</code>): The input data.</li> <li>code_system_col (<code>str</code>, optional): Column name representing the code system. Defaults to 'code_system'.</li> <li>patientid_col (<code>str</code>, optional): Column name representing the patient ID. Defaults to 'patientid'.</li> <li>resolution (<code>float</code>, optional): Granularity of the community detection. Defaults to 1.0.</li> <li>random_state (<code>int</code>, optional): Seed for the random number generator. Defaults to 42.</li> <li>output_col (<code>str</code>, optional): Column name where the module numbers will be saved. Defaults to 'module_number'.</li> </ul>"},{"location":"modularity_encoding/api/#returns","title":"Returns:","text":"<ul> <li>tuple:</li> <li>G (<code>nx.Graph</code>): The code system network graph.</li> <li>data (<code>pd.DataFrame</code>): Updated data with a new column containing module numbers for each code system.</li> </ul> <pre><code>import pandas as pd\n# Import the dataset (example dataset)\ndata = pd.DataFrame({\n    'patientid': [1, 1, 2, 2],\n    'code_system': ['J15', 'B24', 'I50', 'F06']\n})\nG, processed_data= me.modularity_encode(data, code_system_col='code_system', patientid_col='patientid', resolution=1.0, random_state=42, output_col='module_number')\n</code></pre> <pre><code>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6/6 [00:00&lt;00:00, 5996.15it/s]\n</code></pre> <p>The first function will do the full work. If the user wants to perform the encoding step by step, the following functions can be followed.</p>"},{"location":"modularity_encoding/api/#the-second-function-create_code_system_network","title":"The second function <code>create_code_system_network</code>","text":"<p>The <code>create_code_system_network</code> function is used to create a network of code systems based on patient data. It takes in a pandas data frame data that contains patient data with a code_system column and returns a NetworkX graph object G. The network is represented in nodes and edges. The nodes are the code of the code_system , for examples ICD-10 codes, and the edges between the nodes will represent the number of patients who had this pair of ICD-10 codes.</p>"},{"location":"modularity_encoding/api/#parameters_1","title":"Parameters:","text":"<p>The function takes 3 arguments:</p> <ul> <li>data (<code>pd.DataFrame</code>): the defined dataset</li> <li>code_system_col (<code>str</code>, optional): which defines the column contain the code system </li> <li>patientid_col (<code>str</code>, optional): which defines the patients ids in a longitudinal format. </li> </ul> <p>The function will count the number of patients combined each pair of code system codes and define that as the edges of the network while the nodes wil be the code system codes themselves as mentioned before. The fuction can be called as follows:</p> <pre><code>G = me.create_code_system_network(data, code_system_col='code_system', patientid_col='patientid')\n</code></pre> <pre><code>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6/6 [00:00&lt;?, ?it/s]\n</code></pre> <p>The resulted network (G) is a NetworkX object and different commands of NetworkX can be applied on it by importing NetworkX module.</p>"},{"location":"modularity_encoding/api/#the-third-function-detect_code_system_communities","title":"The third function  <code>detect_code_system_communities</code>","text":"<p>The <code>detect_code_system_communities</code> function is used to detect communities within the code system network. It takes in a NetworkX graph object G and an optional random_state parameter for reproducibility and returns a dictionary code_system_to_module that maps each code system to its corresponding module number. The package uses the Louvain method described in Fast unfolding of communities in large networks, Vincent D Blondel, Jean-Loup Guillaume, Renaud Lambiotte, Renaud Lefebvre, Journal of Statistical Mechanics: Theory and Experiment 2008(10), P10008 (12pp). The package \"community\" https://github.com/taynaud/python-louvain and https://python-louvain.readthedocs.io/en/latest/api.html is used to detect the modules.</p> <p>The modules can be interpreted as a cluster of code of code system which has denser connection among them than the rest of the network. As we build the network on the occurrences of these codes between patients, the modules can be interpreted as these codes of the code system tend to frequently cooccur in reality. For example if we use ICD codes (diagnoses), the modules will reveal the multimorbidity patterns in the dataset population, whereas if we use the ATC codes (medications), the modules will represent the comedication patterns in the dataset population.</p>"},{"location":"modularity_encoding/api/#parameters_2","title":"Parameters:","text":"<ul> <li>G:the network created in the first function <code>create_code_system_network</code></li> <li>random_state:int, default(None), (optional) The parameter will assign a fixes random_state value.</li> <li>resolution: double, default =1, (optional).  The resolution of modularity detection can be modified in the paramerte \"resolution\" which will change the size of modules.  The default value is 1. represents the time described in \u201cLaplacian Dynamics and Multiscale Modular Structure in Networks\u201d, R. Lambiotte, J.-C. Delvenne, M. Barahona.</li> </ul> <p>The function can be called as follows:</p> <pre><code>modules = me.detect_code_system_communities(G, random_state=42, resolution=1)\nmodules\n</code></pre> <pre><code>{'B24': 0, 'J15': 0, 'F06': 1, 'I50': 1}\n</code></pre>"},{"location":"modularity_encoding/api/#the-fourth-function-encode_code_system_to_module","title":"The fourth function <code>encode_code_system_to_module</code>","text":"<p>The <code>encode_code_system_to_module</code> function is used to encode the code systems in a pandas data frame data to their corresponding module numbers based on the \"code_system_to_module\" dictionary returned from \"detect_code_system_communities\". It returns the original dataset with an additional column which indicates the module number this code belong to in the network.</p>"},{"location":"modularity_encoding/api/#parameters_3","title":"Parameters:","text":"<ul> <li>data: the network created in the first function <code>create_code_system_network</code></li> <li>modules: defines the modules detected in the second function \"detect_code_system_communities\"</li> <li>output_col: specifies the desired name of the added column, by default the column name is module_numnber unless changed.</li> </ul> <p>It is imporatant to change the column name after the argument \"output_col\" if the used will perform modularity encoding on more than one code system in the same dataset to prevent overwriting the same column. </p> <p>The function can be called as follows:</p> <pre><code>encoded_data = me.encode_code_system_to_module(data, modules, output_col='my_module_number')\nencoded_data\n</code></pre> patientid code_system module_number my_module_number 0 1 J15 0 0 1 1 B24 0 0 2 2 I50 1 1 3 2 F06 1 1 <p>Once the user have this column, it can be used in the model as it will have much less categories (dimensions), alternativlely it can be dummy encoded to binary variables or by other methods of encoding and won't cause much dispersity in the dataset.</p>"},{"location":"modularity_encoding/api/#the-fifth-function-print_edge_list","title":"The fifth function <code>print_edge_list</code>","text":"<p>The function <code>print_edge_list</code> will save the the network's edgelist in a csv file in the same work directory. The funtion takes two arguments: </p> <ul> <li>G (<code>nx.Graph</code>): the graph object (G) </li> <li>The CSV filename (<code>pd.DataFrame</code>), example (edgelist.csv)</li> </ul> <p>The function can be called as follows:</p> <pre><code>print_edge_list = me.print_edge_list(G, \"edgelist.csv\")\n</code></pre> <pre><code>Edge list saved to file: edgelist.csv\n</code></pre> <pre><code>edgelist = pd.read_csv('edgelist.csv')\nedgelist.head()\n</code></pre> source target weight 0 B24 J15 1 1 F06 I50 1"},{"location":"modularity_encoding/api/#the-sixth-function-assign_module","title":"The sixth function <code>assign_module</code>","text":"<p>The <code>assign_module</code> function maps a given code (or a list of codes) from the studied code system to its corresponding module. This function is especially useful when the user wishes to make new predictions and needs to find the corresponding encoding integer of a code in the code system.</p>"},{"location":"modularity_encoding/api/#parameters_4","title":"Parameters:","text":"<ul> <li>codes (<code>str</code>): The code(s) from the code system that the user wants to map to a module. These codes should exist in the original dataset.</li> <li>modules (<code>dict</code>): A dictionary containing mappings from codes to modules, generated from the function <code>detect_code_system_communities</code>.</li> </ul>"},{"location":"modularity_encoding/api/#returns_1","title":"Returns:","text":"<ul> <li>If the code(s) exists in the dictionary, the function returns the corresponding module.</li> <li>If the code(s) does not exist in the dictionary, the function returns an appropriate error message indicating that the code is not present in the code-system-to-module mapping.</li> </ul>"},{"location":"modularity_encoding/api/#assumptions","title":"Assumptions","text":"<ul> <li>The function assumes that the code entered is a string value and that it presented in the code system to module mapping (i.e. the original dataset).</li> </ul> <p>The function can be called as follows:</p> <pre><code># for one code \ncode = \"J15\" # example\nmodule_of_the_code = me.assign_module(code, modules) \nprint('The module containig this code is module:',module_of_the_code)\n</code></pre> <pre><code>The module containig this code is module: 0\n</code></pre> <pre><code># for multiple codes\ncodes = [\"J15\", \"B24\", \"A11\"] # example\nmodule_of_the_codes = me.assign_module(codes, modules) \nprint('The modules list containing these codes:',module_of_the_codes)\n</code></pre> <pre><code>The modules list containing these codes: [0, 0, \"The entered code 'A11' is not present in the code system to module mapping.\"]\n</code></pre>"},{"location":"modularity_encoding/api/#user-example","title":"User example","text":"<p>Here's a full example of how to use the <code>modularity_encoding</code> package:</p> <pre><code>import modularity_encoding.functions as me\nimport pandas as pd\n\n# import you dataset\ndata = pd.DataFrame({\n    'patientid': [1, 1, 2, 2],\n    'code_system': ['J15', 'B24', 'I50', 'F06']\n})\n\n# one function \nG, processed_data= me.modularity_encode(data, code_system_col='code_system', patientid_col='patientid', resolution=1.0, random_state=42, output_col='module_number')\n# save the data with the new column\nprocessed_data.to_csv('processed_data.csv', index=False)\n# print or save the edgelist\nme.print_edge_list(G, \"edgelist.csv\")\n\n# Consecutive functions (must be run in this order)\n# create the network \nG = me.create_code_system_network(data, code_system_col='code_system', patientid_col='patientid')\n\n# detect the modules in the network \nmodules = me.detect_code_system_communities(G, random_state=42,resolution=1) # change resolution if needed\n\n# assign the codes of the targeted code system to their correponding module id \nencoded_data = me.encode_code_system_to_module(data, modules, output_col='my_module_number')\n\n# print the data\nprint(data)\n\n# print th network's edgelist  \nprint_edge_list = me.print_edge_list(G, \"edgelist.csv\")\n# in case of implementing the function on other code system in the same dataset remember to specify the new \"code_system_col\" \n# and to change the \"output_col\" name. \n\n# To make new prediction the user can use this function to map the code in the code system to its correponding module\n# map codes' modules to make new predictions\ncode = \"J15\"\nmodule_of_the_code = me.assign_module(code, modules) \nprint(module_of_the_code)\n\n# for multiple codes\ncodes = [\"J15\", \"B24\", \"A11\"] # example\nmodule_of_the_codes = me.assign_module(codes, modules) \nprint('The modules list containing these codes:',module_of_the_codes)\n</code></pre> <pre><code>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6/6 [00:00&lt;?, ?it/s]\n\n\nEdge list saved to file: edgelist.csv\n\n\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6/6 [00:00&lt;?, ?it/s]\n\n   patientid code_system  module_number  my_module_number\n0          1         J15              0                 0\n1          1         B24              0                 0\n2          2         I50              1                 1\n3          2         F06              1                 1\nEdge list saved to file: edgelist.csv\n0\nThe modules list containing these codes: [0, 0, \"The entered code 'A11' is not present in the code system to module mapping.\"]\n</code></pre>"},{"location":"modularity_encoding/api/#faq","title":"FAQ","text":"<p>1. What is this package used for?</p> <p>This package provides a set of functions to create a network from patient data, detect communities in the network, and encode the code systems to their corresponding module numbers. It can be used for analyzing patterns and relationships among different medical code systems in patient data.</p> <p>2. What are some common use cases for this package?</p> <p>The package can be used to group any code system that can be aggragted on a level (for example the patient level). It is especially useful for high-dimensional code systems such as the International Classification of Diseases (ICD) codes, Anatomical Therapeutic Chemical (ATC) codes, Diseases Related Group (DRG) codes in the health domain, Current Procedural Terminology (CPT) codes, and Systematized Nomenclature of Medicine (SNOMED) codes. </p> <p>3. What is a code system?</p> <p>A code system is a standardized system of codes used to represent medical concepts, such as diagnoses, procedures, and medications. Examples of code systems include ICD-10, CPT, ATC, RxNorm, etc.</p> <p>4. What is a network?</p> <p>In the context of this package, a network is a graph representation of the relationships among different code systems based on the number of shared patients. Each code system is a node, and the number of shared patients between two code systems is the weight of the edge between them.</p> <p>5. What is a community in a network?</p> <p>A community in a network is a group of nodes that are more densely connected to each other than to the rest of the network. In the context of this package, a community represents a group of code systems that tend to co-occur in patients more frequently than with the other codes in the code systems.</p> <p>6. What is module encoding?</p> <p>Module encoding is the process of mapping each code system to its corresponding community module number. This allows for easier analysis and visualization of the relationships among code systems in the network.</p> <p>7. How do I install this package?</p> <p>The package can be installed using <code>pip</code> install. The package dependencies (see above) have to be installed as well.</p> <p>8. What format does the input data need to be in?</p> <p>The input data needs to be a Pandas DataFrame with columns for patient IDs and code systems. The column names for these two columns can be specified as arguments to the <code>create_code_system_network</code> function.</p> <p>9. What is the output of the <code>detect_code_system_communities</code> function?</p> <p>The output of this function is a dictionary that maps each code system to its corresponding community module number.</p> <p>10. What is an edge list?</p> <p>An edge list is a way of representing the edges (connections) between nodes (points) in a network. It is a table with three columns representing tha pair of nodes that are connected in the network and a third column representing the weight of the connections, which in this case is the number of patients who share the nodes. The edge list can be used to create a visualization of the network or to perform further analysis using other software tools.</p> <p>11. How do I save the network as an edge list?</p> <p>The <code>print_edge_list</code> function to save the network as an edge list in a CSV file in the same working directory. By default, the CSV file name is \"edgelist.csv\" but the name can be specified as an argument in the function.</p> <p>12. How can I implement other network measures on the created Netwrok?</p> <p>The created Network is a NetworkX object , which means that all measures implemented in NetworkX package can be applied on the created network by importing NetworkX.</p> <p>13. How can I visualize the network created by the <code>create_code_system_network</code> function?</p> <p>As mentioned, the function returns a NetworkX graph object, which can be visualized using the built-in plotting functions provided by the networkx and matplotlib libraries. You can also export the graph as an edge list using the print_edge_list() function and use external tools to create custom visualizations such as Gephi.</p> <p>14. How can I interpret the module numbers assigned by the <code>detect_code_system_communities</code> function?</p> <p>The <code>detect_code_system_communities</code> function uses the Louvain method to identify groups of related code systems based on their shared patients. Each module is assigned a unique integer label, which can be used to group code systems together for further analysis. The meaning of the module labels will depend on the specific dataset and application and the used code system.</p> <p>15. Can this package be used with data from any type of healthcare provider or system?</p> <p>Yes, as long as the data is in a format that can be read by Pandas and the necessary columns are included in the input data.</p> <p>16. Are there any limitations to the size of dataset that can be analyzed using this package?</p> <p>The size of the dataset that can be analyzed will depend on the hardware available and the specific use case. However, this package includes optimizations such as using <code>NetworkX</code> graphs to efficiently represent and analyze the data, and using <code>tqdm</code> library to display progress bars during processing.</p> <p>17.In case of making new predictions, how can I determine the assigned module for the code system?</p> <p>You can use the <code>assign_module</code> function to map the code to its correpoding module interger id (corresponds to the encdoing number of this code). The code entered must be a string value and must have been in the original dataset.</p>"},{"location":"modularity_encoding/usage/","title":"Usage Guide","text":""},{"location":"modularity_encoding/usage/#dependencies","title":"Dependencies","text":"<p>The <code>modularity_encoding</code> package relies on some external libraries, which need to be installed first:</p> <ul> <li><code>pandas</code>: Used for handling and manipulating the dataset.</li> <li><code>networkx</code>: Facilitates the creation and manipulation of complex networks of code systems.</li> <li><code>itertools</code>: Helps in efficient looping for combinations and permutations needed in module detection.</li> <li><code>python-louvain</code>: Essential for detecting communities within the network.</li> <li><code>tqdm</code>: Provides progress bars to loops to visualize the computation time.</li> <li><code>matplotlib</code>: Required for plotting networks and other visualizations to understand the data better.</li> </ul>"},{"location":"modularity_encoding/usage/#installation","title":"Installation","text":"<p>The <code>modularity_encoding</code> package can be installed using <code>pip</code>:</p> <pre><code>!pip install modularity_encoding==0.1\n</code></pre>"},{"location":"modularity_encoding/usage/#importing","title":"Importing","text":"<p>The package is imported as follows:</p> <pre><code>import modularity_encoding.functions as me\n</code></pre>"},{"location":"mydataviewer-GUI/","title":"Overview","text":""},{"location":"mydataviewer-GUI/#my-dataviewer-package","title":"<code>My DataViewer</code> Package \ud83d\udda5\ufe0f","text":"<p><code>My DataViewer</code> : A Lightweight Real-Time Interactive Data Viewer IDE for Python</p> <p>Inspired by the Stata <code>browser</code>,<code>My DataViewer</code> is a lightweight, interactive data viewer IDE for Pyhton built with <code>PyQt5</code>, designed for real-time manipulation and exploration of pandas DataFrames. Featuring real-time reflection of changes to your datasets, <code>My DataViewer</code> offers a lightweight, intuitive way to explore and analyze full dataframes.  </p> <p>An Overview of <code>browse</code> in Stata - <code>browse</code> in Stata opens a window that displays your dataset in a spreadsheet-like view. It allows you to see the dataset, scroll through it, and interactively examine variables and their values.</p> <p>Interactivity in Stata's <code>browse</code>:</p> <ul> <li> <p>Live, Real-time Reflection: Any changes you make to the dataset (e.g., adding variables, modifying values) are automatically reflected in the <code>browse</code> window without needing to close and reopen the window.</p> </li> <li> <p>Read-Only by Default: The browse command is read-only by default, so you can\u2019t modify values directly in the viewer unless you switch to edit mode.</p> </li> <li> <p>View Subsets of Data: You can specify subsets of the data to view (e.g., certain variables or observations).</p> </li> </ul> <p>Integrated features in <code>My DataViewer</code> package:</p> <ul> <li>Real-time dataset updates: Automatically reflects changes in the DataFrame as you manipulate it.</li> <li>Embedded <code>IPython console</code>: Execute Python code and interact with your data directly within the viewer.</li> <li>Search and filter dataframe: Easily search and filter your data, with results reflected in real time.</li> <li>Check and scroll through the whole dataframe: Allows the user to inspect the whole dataframe ensuring the changes are correctly executed.</li> <li>Custom themes and fonts: Switch between selected themes and fonts for a personalized experience.</li> <li>Split-view layout: Includes a variable table and the main data view, allowing for easy navigation of variables and their types.</li> </ul> <p>A quick comaprison between Stata <code>browse</code> and RStudio <code>View()</code></p> Feature Stata browse RStudio View() Real-time reflection of changes Yes \u2014 automatically updates with changes No \u2014 requires a manual refresh (View() again) Read-Only by default Yes (but can switch to edit mode) Yes (no edit mode available) Interactive editing Yes, if you switch to edit mode No Filtering/Subset view Yes, allows viewing subsets of variables/rows Requires filtering beforehand, not interactive Customizable views Yes, can display specific columns/rows Requires code to subset data, then view Viewer format Standalone window (outside the main Stata UI) Integrated as a tab within RStudio"},{"location":"mydataviewer-GUI/#a-comparison-with-stata-browse-rstudio-view-and-my-dataviewer","title":"A comparison with Stata <code>browse</code>, RStudio <code>View()</code>, and <code>My DataViewer</code>","text":"Feature Stata <code>browse</code> RStudio <code>View()</code> <code>My DataViewer</code> package Real-time reflection of changes Yes \u2014 automatically updates with data changes No \u2014 must manually refresh the view Yes \u2014 real-time updates are supported via observable DataFrame Read-Only by default Yes (can switch to <code>edit</code> mode) Yes (no direct editing in the viewer) Yes, read-only but with real-time updates Interactive editing Yes, if you switch to <code>edit</code> mode No No direct editing in the viewer, editing must be done programmatically Viewing subsets of data Yes, easily view subsets of variables/rows Yes, but requires filtering the data manually Yes, filtering implemented using search functionality in real time Window integration Standalone window (outside Stata\u2019s main interface) Integrated tab in RStudio Integrated window in PyQt5, with split views and dockable widgets Sorting and filtering in viewer Yes, sortable columns and filter rows interactively Limited sorting/filtering functionality Yes, with header-click sorting and real-time search and filtering Customizable views Yes, can select specific columns/rows Requires prior filtering in code Customizable through filtering and real-time search features Console integration No embedded console No embedded console Yes, embedded IPython console for live data manipulation Custom themes No No Yes, switchable light/dark themes and custom fonts Jupyter integration No No Yes, updates Jupyter kernel namespace if running inside Jupyter"},{"location":"mydataviewer-GUI/#license","title":"License","text":"<p>Released under the MIT License: Copyright (C) 2024 <code>mydataviewer-GUI</code></p> <p>Developed by: Mohsen Askar  ceaser198511@gmail.com</p>"},{"location":"mydataviewer-GUI/api/","title":"API Reference","text":""},{"location":"mydataviewer-GUI/api/#my-dataviewer-components","title":"<code>My DataViewer</code> components","text":"<ol> <li><code>data_import_dialog.py</code>: Manages data import imports using PyQt. The imported data could be in \".csv\", \".excel\", \".json\", \".sql\", \".api\", or \".dta\".</li> <li><code>data_viewer.py</code>: Contains the <code>DataViewer</code> class, which provides the core GUI using PyQt. It handles table views, search functionality, variable tables, themes, fonts, and a console.</li> <li><code>dataframe_model.py</code>: Defines <code>DataFrameModel</code>, which is the model for the table view and supports pagination, sorting, and updating based on an observable DataFrame.</li> <li><code>observable_dataframe.py</code>: Extends <code>pandas.DataFrame</code> into an <code>ObservableDataFrame</code> that triggers callbacks on changes, allowing the UI to stay in sync.</li> <li><code>view_dataframe.py</code>: Defines the main <code>view_dataframe</code> function that launches the DataViewer with the provided DataFrame, handling the GUI's event loop.</li> <li><code>themes.py</code>: Contains style sheets for light, dark, and solarized dark themes, which are applied to the PyQt widgets.</li> </ol>"},{"location":"mydataviewer-GUI/api/#working-with-jupyter-qtconsole","title":"Working with Jupyter QtConsole","text":"<ul> <li>As stated in its documentation \"The Qt console is a very lightweight application that largely feels like a terminal, but provides a number of enhancements only possible in a GUI, such as inline figures, proper multi-line editing with syntax highlighting, graphical calltips, and much more.\"</li> <li>The embedded IPython console provides an interactive environment to manipulate your DataFrame. You can perform operations such as filtering rows, applying transformations, or generating statistics directly from within the console. Any changes made in the console are immediately reflected in the table view, ensuring real-time feedback</li> <li>The user can activate the desired environment using <code>conda activate</code>.</li> <li>By default the loaded dataframe is named <code>df</code>.</li> <li>See the documentaion here: https://qtconsole.readthedocs.io/en/stable/</li> </ul>"},{"location":"mydataviewer-GUI/api/#additional-resources","title":"Additional Resources","text":"<ul> <li>PyQt5 Documentation: https://www.riverbankcomputing.com/static/Docs/PyQt5/</li> <li>qtconsole Documentation: https://qtconsole.readthedocs.io/en/latest/</li> </ul>"},{"location":"mydataviewer-GUI/usage/","title":"Usage Guide","text":""},{"location":"mydataviewer-GUI/usage/#1-my-dataviewer-package-installation","title":"1. <code>My DataViewer</code> package installation","text":"<p>Install from PyPI using <code>pip</code></p> <pre><code>!pip install mydataviewer-GUI\n</code></pre>"},{"location":"mydataviewer-GUI/usage/#2-package-dependencies","title":"2. Package dependencies","text":"<ul> <li>Python 3.6 or newer</li> <li>Pandas</li> <li>PyQt5</li> <li>qtconsole</li> <li>ipykernel</li> <li>jupyter_client</li> </ul>"},{"location":"mydataviewer-GUI/usage/#3-how-to-start-my-dataviewer","title":"3. How to start <code>My DataViewer</code>","text":""},{"location":"mydataviewer-GUI/usage/#a-running-the-package-from-the-terminal-using-viewdata-a-command","title":"A. Running the Package from the Terminal using <code>viewdata</code> a command","text":"<p>After installing the package using <code>pip install mydataviewer</code>, you can launch the <code>My DataViewer</code> directly from the terminal using the command-line interface (CLI).</p>"},{"location":"mydataviewer-GUI/usage/#steps","title":"Steps:","text":"<ol> <li>Open a terminal window.</li> <li>Activate the environment in which <code>My DataViewer</code> is installed, <code>conda activate my_env</code>.</li> <li>Run the following command to start the <code>My DataViewer</code>:</li> </ol> <pre><code>viewdata\n</code></pre>"},{"location":"mydataviewer-GUI/usage/#b-running-my-dataviewer-from-the-terminal-using-a-python-file","title":"B. Running <code>My DataViewer</code> from the terminal using a python file","text":"<p>You can also launch the <code>My DataViewer</code> directly from the terminal by running a custom python file.</p>"},{"location":"mydataviewer-GUI/usage/#steps_1","title":"Steps:","text":"<ol> <li>Create a python file, for example: 'my_file.py' or any name the user chooses.</li> <li>In the file type these lines:</li> </ol> <pre><code>from mydataviewer.view_dataframe import view_dataframe\nview_dataframe()\n</code></pre> <ol> <li>In a terminal window, navigate to the directory of 'my_file.py'.</li> <li>Type 'python my_file.py', and the package will start.</li> </ol>"},{"location":"mydataviewer-GUI/usage/#c-running-my-dataviewer-from-vs-code","title":"C. Running <code>My DataViewer</code> from VS Code","text":"<p>You can also run the package from VS Code using the terminal.</p> <ol> <li>Open your project in VS Code.</li> <li>Open the integrated terminal by going to View &gt; Terminal.</li> <li>Type the <code>viewdata</code> and press <code>Enter</code>:</li> </ol> <p>This will start the <code>My DataViewer</code> like in a regular terminal.</p>"},{"location":"mydataviewer-GUI/usage/#d-running-my-dataviewer-from-an-exe-file","title":"D. Running <code>My DataViewer</code> from an .exe file","text":"<p>You can also run the package by navigating to <code>Scripts</code> inside the environment in which <code>My DataViewer</code> is insalled.</p> <ol> <li>Navigate to directory <code>C:\\Users\\USER_NAME\\Anaconda3\\envs\\ENV_NAME\\Scripts</code></li> <li>You will find <code>viewdata.exe</code> file, open it.</li> </ol> <p>This will directly start the <code>My DataViewer</code>. The user can also copy the <code>viewdata.exe</code> file to desktop or anyother directory to access it easily.</p>"},{"location":"mydataviewer-GUI/usage/#4-interacting-with-the-my-dataviewer","title":"4. Interacting with the <code>My DataViewer</code>","text":"<p>After running the <code>viewdata</code> command, a data import box will open allowing the user to locate and open the dataframe from <code>Import Dataframe</code> menu. After choosing the dataframe, the <code>My DataViewer</code> GUI window will open, displaying the DataFrame. The window includes:</p> <ul> <li>Table View: Displays the DataFrame in a table format.</li> <li>Sidebar: Shows variable names and data types.</li> <li>Search Bar: Allows you to search within the DataFrame.</li> <li>Interactive Console: An embedded IPython console at the bottom, enabling you to manipulate the DataFrame and reflect the changes in real-time.</li> </ul>"},{"location":"mydataviewer-GUI/usage/#5-customizing-my-dataviewer","title":"5. Customizing <code>My DataViewer</code>","text":"<p>Themes and Fonts - The <code>My DataViewer</code> includes <code>themes</code> and <code>fonts</code> menu options for changing fonts and themes. Access these from the menu bar at the top of the DataViewer window.</p> <p>Themes - Light Theme (Default) - Dark Theme - Solarized Dark Theme - Solarized Light Theme - Monokai Theme - Shades of Purple Theme - Vue Theme</p> <p>Fonts - Arial - Courier New - Times New Roman - Georgia - Verdana - Consolas</p>"},{"location":"mydataviewer-GUI/usage/#watch-my-dataviewer-video-demo","title":"Watch My Dataviewer video demo:","text":""},{"location":"stata_codebook/","title":"Overview","text":""},{"location":"stata_codebook/#stata-codebookpackage","title":"<code>Stata Codebook</code>Package \ud83d\udcca","text":"<p>The <code>stata_codebook</code> package provides tools for generating detailed descriptive statistics and summaries of data frames, similar to Stata's <code>codebook</code> command. <code>codebook</code> command is a very useful command to examine dataset varaibles.  In Stata documentation \"<code>codebook</code> examines the data in producing its results. For variables that codebook thinks are continuous, it presents the mean; the standard deviation; and the 10th, 25th, 50th, 75th, and 90th percentiles. For variables that it thinks are categorical, it presents a tabulation.\".</p> <p>The package supports various features, including: - Summary statistics for numeric and categorical variables - Handling of columns with missing values - Detection of mixed data types - Normality testing with Shapiro-Wilk or Kolmogorov-Smirnov tests, depending on dataset size - Output formatting for academic or professional reports - Check for embedded, leading, and trailing balnks in the variables.</p>"},{"location":"stata_codebook/#why-use-stata_codebook-over-built-in-summary-statistics","title":"Why use stata_codebook over built-in summary statistics?","text":"<p>While pandas offers built-in functions like <code>describe()</code> and <code>value_counts()</code> for summarizing data, the <code>codebook package</code> provides several advantages:</p> <ol> <li> <p>Comprehensive Overview</p> <ul> <li> <p>Numeric and Categorical Data: Unlike <code>describe()</code>, which primarily focuses on numeric data, <code>codebook</code> provides a detailed summary of both numeric and categorical variables. It not only gives you the common statistics like mean, median, and standard deviation but also includes the top categories and their proportions for categorical variables.</p> </li> <li> <p>Handling of Missing Values: The <code>codebook</code> function provides a clear count of missing values for each variable, which is not directly offered by the <code>describe()</code> function.</p> </li> </ul> </li> <li> <p>Data Quality Checks</p> <ul> <li> <p>Detection of Blanks: One of the unique features of the <code>codebook</code> function is its ability to detect embedded, leading, and trailing blanks in string data. This can be crucial for identifying and resolving data entry issues that might otherwise go unnoticed with standard summary statistics.</p> </li> <li> <p>Mixed Data Types: If a column contains mixed data types, the function will automatically detect and handle it, issuing warnings to alert you to potential data quality problems.</p> </li> </ul> </li> <li> <p>Advanced Statistical Insights</p> <ul> <li>Normality Testing: The <code>codebook</code> function includes normality testing (Shapiro-Wilk for small datasets (&lt;5000 observations) and Kolmogorov-Smirnov for large datasets), providing you with p-values that can help you assess the distribution of your numeric data. This goes beyond what the standard <code>describe()</code> function offers.</li> <li>Confidence Intervals: In advanced mode, the function calculates 95% confidence intervals for both numeric variables and the proportions of the top categories in categorical variables, offering deeper insights into your data's variability.</li> </ul> </li> <li> <p>Customizable and Readable Output</p> <ul> <li>Formatted Output: The <code>codebook</code> function rounds numerical results to a specified number of decimal places, ensuring that the output is easy to read and interpret. This is especially valuable for creating reports or presentations where clarity and professionalism are paramount.</li> <li>Consistent Display: By returning a DataFrame with all relevant statistics neatly organized, <code>codebook</code> makes it easier to compare variables side by side, which can be inefficient when using multiple pandas functions.</li> </ul> </li> <li> <p>Easy to Use</p> <ul> <li>Single Command: With just one command, you can generate a detailed and well-rounded summary of one column or the entire DataFrame, saving time and reducing the risk of overlooking important details.</li> </ul> </li> </ol>"},{"location":"stata_codebook/#license","title":"License","text":"<p>Released under the MIT License: For more details, see the <code>LICENSE</code> file. Copyright (C) 2024 <code>stata_codebook</code></p> <p>Developed by: Mohsen Askar ceaser198511@gmail.com</p> <p>Citation</p> <p>If you use Stata Codebook in your research, please cite: <pre><code>@software{stata_codebook2024,\n    title = {AssumptionSheriff: A Python Package for Statistical Assumption Checking},\n    author = {Mohsen Askar},\n    e-mail = {ceaser198511@gmail.com},\n    year = {2024},\n    url = {https://pypi.org/project/stata_codebook/}\n}\n</code></pre></p>"},{"location":"stata_codebook/api/","title":"API Reference","text":""},{"location":"stata_codebook/api/#detailed-function-documentation","title":"Detailed Function Documentation","text":""},{"location":"stata_codebook/api/#function-codebook","title":"Function: <code>codebook</code>","text":"<p>Generates a detailed codebook for a given DataFrame/variable in the dataframe, providing descriptive statistics and data quality checks.</p> <p>Parameters:</p> <ul> <li><code>df</code> (pandas.DataFrame): The DataFrame to analyze.</li> <li><code>column</code> (str, optional): If specified, only this column will be analyzed. Defaults to <code>None</code>.</li> <li><code>advanced</code> (bool, optional): If <code>True</code>, includes additional statistics like standard deviation, confidence intervals, and normality tests. Defaults to <code>False</code>.</li> <li><code>decimal_places</code> (int, optional): The number of decimal places to round numerical results. Defaults to 3.</li> </ul> <p>Returns: - pandas.DataFrame: A DataFrame containing the codebook with descriptive statistics and data quality checks.</p> <p>Example Usage:</p> <pre><code># Generate an advanced codebook for a specific column\ncodebook(df, column='age', advanced=True, decimal_places=2)\n</code></pre> Variable Type Unique values Missing values Blank issues Range 25th percentile 50th percentile (Median) 75th percentile Mean Examples Top categories SD 95% CI Normality test p-value (normality) 0 age float64 4 1 Not applicable (25.0, 40.0) 28.75 32.5 36.25 32.5 [35.0, 25.0, 30.0] - 6.45 (26.18, 38.82) Shapiro-Wilk 0.97"},{"location":"stata_codebook/api/#notes","title":"Notes","text":"<p>If a column contains all missing values, the function will skip detailed analysis for that column and indicate that it is entirely missing. The function automatically handles mixed data types by converting the column to an object type and issuing a warning.</p>"},{"location":"stata_codebook/api/#output-explanation","title":"Output Explanation:","text":"<ul> <li>Variable: The name of the variable.</li> <li>Type: The data type of the variable.</li> <li>Unique values: The number of unique non-null values.</li> <li>Missing values: The number of missing (null) values.</li> <li>Blank issues: Any detected issues with leading, trailing, or embedded blanks in string variables.</li> <li>Range: The minimum and maximum values for numeric variables.</li> <li>25th, 50th, 75th percentile: The respective percentiles for numeric variables.</li> <li>Mean: The mean of numeric variables.</li> <li>SD: The standard deviation for numeric variables (advanced mode).</li> <li>95% CI: The 95% confidence interval for numeric variables (advanced mode).</li> <li>Normality test: The type of normality test applied (Shapiro-Wilk (for datasets with 5000 or fewer observations) or Kolmogorov-Smirnov (for larger datasets)).</li> <li>p-value (normality): The p-value from the normality test.</li> <li>Top categories: The most frequent categories for categorical variables.</li> <li>Top category proportion: The proportion of the top category for categorical variables (advanced mode).</li> <li>95% CI (top category): The 95% confidence interval for the top category proportion (advanced mode).</li> </ul>"},{"location":"stata_codebook/api/#faqtroubleshooting","title":"FAQ/Troubleshooting","text":"<p>Q1: The codebook function isn't working for my DataFrame with mixed data types. What should I do?</p> <p>A: The <code>codebook</code> function automatically detects and converts columns with mixed data types to object (string) type. If you see a warning about mixed types, ensure your data is clean and consistently typed, or allow the function to handle it automatically.</p> <p>Q2: Why does the function skip some columns?</p> <p>A: The function may skip columns if they contain all missing values (<code>NaN</code>). The output will indicate if a column is entirely missing.</p> <p>Q3: How can I adjust the number of decimal places for numerical results?</p> <p>A: You can adjust the decimal precision by setting the <code>decimal_places</code> parameter when calling the <code>codebook</code> function:</p> <p><code>codebook(df, advanced=True, decimal_places=2)</code></p>"},{"location":"stata_codebook/usage/","title":"Usage Guide","text":""},{"location":"stata_codebook/usage/#1-installation","title":"1. Installation","text":"<p>The package can be installed directly from PyPI using pip:</p> <p><code>pip install stata_codebook</code></p>"},{"location":"stata_codebook/usage/#2-quick-start","title":"2. Quick Start","text":"<p>Here's a quick example to get you started:</p> <pre><code>import pandas as pd\nfrom stata_codebook import codebook\n</code></pre> <pre><code># Sample DataFrame\ndata = {\n    'age': [25, 30, 35, 40, None],\n    'income': [50000, 60000, 70000, 80000, 90000],\n    'gender': ['Male', 'Female', 'Female', 'Male', None],\n    'is_employed': [True, True, False, True, None]\n}\ndf = pd.DataFrame(data)\n</code></pre> <pre><code># codebook for all dataset varaibles\ncodebook(df)\n</code></pre> Variable Type Unique values Missing values Blank issues Range 25th percentile 50th percentile (Median) 75th percentile Mean Examples Top categories SD 95% CI Normality test p-value (normality) Top category proportion 95% CI (top category) 0 age float64 4 1 Not applicable (25.0, 40.0) 28.75 32.5 36.25 32.5 [35.0, 25.0, 30.0] - - - - - NaN NaN 1 income int64 5 0 Not applicable (50000, 90000) 60000.0 70000.0 80000.0 70000.0 [70000, 50000, 60000] - - - - - NaN NaN 2 gender object 2 1 No blanks detected - - - - - [Female, Male, Female] {'Male': 2, 'Female': 2} - NaN - - - - 3 is_employed object 2 1 No blanks detected - - - - - [False, True, True] {True: 3, False: 1} - NaN - - - - <pre><code># codebook for specific column in the dataset\ncodebook(df, column='income') # numerical column\n</code></pre> Variable Type Unique values Missing values Blank issues Range 25th percentile 50th percentile (Median) 75th percentile Mean Examples Top categories SD 95% CI Normality test p-value (normality) 0 income int64 5 0 Not applicable (50000, 90000) 60000.0 70000.0 80000.0 70000.0 [70000, 50000, 60000] - - - - - <pre><code># codebook for specific column in the dataset\ncodebook(df, column='gender') # categorical column\n</code></pre> Variable Type Unique values Missing values Blank issues Examples Top categories Range 25th percentile 50th percentile (Median) 75th percentile Mean SD Normality test p-value (normality) Top category proportion 95% CI (top category) 0 gender object 2 1 No blanks detected [Female, Male, Female] {'Male': 2, 'Female': 2} - - - - - - - - - - <pre><code># codebook for specific column in the dataset additional statistics \ncodebook(df, advanced=True)\n</code></pre> Variable Type Unique values Missing values Blank issues Range 25th percentile 50th percentile (Median) 75th percentile Mean Examples Top categories SD 95% CI Normality test p-value (normality) Top category proportion 95% CI (top category) 0 age float64 4 1 Not applicable (25.0, 40.0) 28.75 32.5 36.25 32.5 [35.0, 25.0, 30.0] - 6.455 (26.174, 38.826) Shapiro-Wilk 0.972 NaN NaN 1 income int64 5 0 Not applicable (50000, 90000) 60000.0 70000.0 80000.0 70000.0 [70000, 50000, 60000] - 15811.388 (56140.707, 83859.293) Shapiro-Wilk 0.967 NaN NaN 2 gender object 2 1 No blanks detected - - - - - [Female, Male, Female] {'Male': 2, 'Female': 2} - NaN - - 0.50 (0.01, 0.99) 3 is_employed object 2 1 No blanks detected - - - - - [False, True, True] {True: 3, False: 1} - NaN - - 0.75 (0.326, 1.174)"}]}